# -*- coding: utf-8 -*-
""".Credit Risk Prediction: Decision Trees vs. Logistic Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14UUvoX02PjmQdzUpR4FGuXIehSIxCm2I

# Loan Default Prediction(Decision_Tree VS Logistic Regression)

### Financial institutions face challenges in accurately assessing credit risk before approving loans. A high rate of loan defaults can result in financial losses for lenders. This project aims to build a predictive model using machine learning techniques to classify borrowers as either likely to default or unlikely to default, based on financial indicators such as Annual Income,Credit Score.By leveraging Logistic Regression and Decision Tree Classifiers, this project seeks to enhance the accuracy of credit risk assessments, reducing financial losses and improving lending decisions.
"""



"""## Loading Necessary Libraries"""

import warnings

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from category_encoders import OneHotEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OrdinalEncoder
from sklearn.tree import DecisionTreeClassifier,plot_tree
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline, make_pipeline


warnings.simplefilter(action="ignore", category=FutureWarning)



"""## Exploring the data"""

df=pd.read_csv("train.csv")
df.head()

df.columns

df.info()

df.select_dtypes("number").isna().sum()/len(df)

df.select_dtypes("number").corr()

sns.heatmap(df.select_dtypes("number").corr())

"""### There is an issue of multicollinearity between Bankruptcies and Number of Credit Problems: One needs to be dropped"""

df.select_dtypes("number").hist(figsize=(14, 12), color='blue');

df.select_dtypes("object").head()

df["Home Ownership"].value_counts()



"""## Creating a function for wrangling data"""

# create a function for wrangling data
def wrangle(filepath):
  # Read the csv file
  df=pd.read_csv(filepath)
  # Drop the 'Id' column and the 'Bankruptcies' column(Muliticollinearity)
  df.drop(columns=["Id","Bankruptcies"],inplace=True)
  # Fill Missing Values
  df["Annual Income"]=df["Annual Income"].fillna(df["Annual Income"].median())
  df["Months since last delinquent"]=df["Months since last delinquent"].fillna(df["Months since last delinquent"].mean())
  df["Credit Score"]=df["Credit Score"].fillna(df["Credit Score"].median())
  df["Years in current job"]=df["Years in current job"].fillna(df["Years in current job"].mode()[0])
  # Group the 'Purpose' column to reduce High Cardinality
  rare_categories = df["Purpose"].value_counts()[df["Purpose"].value_counts() < 20].index
  df["Purpose"] = df["Purpose"].replace(rare_categories, "Other")
  # Drop Duplicates
  df.drop_duplicates(inplace=True)
  # Return the clean dataframe
  return df

"""## Applying the wrangle Function to the Data"""

df_clean=wrangle("train.csv")

df_clean.info()

"""## Explalatory Data Analysis"""

## Explalatory Data Analysis

"""## Plotting Default Class structure"""

df_clean["Credit Default"].value_counts(normalize=True).plot(kind="bar");

majority_class_prop, minority_class_prop =df_clean['Credit Default'].value_counts(normalize=True)
print(majority_class_prop, minority_class_prop)

"""## Buid Model

### Baseline Model
"""

acc_baseline =majority_class_prop
print("Baseline Accuracy:", round(acc_baseline, 2))

## split the data

target="Credit Default"
y_train=df_clean[target]
X_train=df_clean.drop(columns=target)

X_train.info()

## read and clean th test Data

test_df=wrangle("test.csv")
test_df.head()

df1 = pd.read_csv("target.csv", nrows=2500)

y_test=df1["flag"]
X_test=test_df
y_test = y_test.iloc[:2500]
X_test = X_test.iloc[:2500]
y_test.info()

X_test.info()

"""## Scaling The Data"""

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train.select_dtypes("number"))
X_test_scaled = scaler.transform(X_test.select_dtypes("number"))

"""## Instantiate Model: Logistic Regression"""

model =make_pipeline(
    OneHotEncoder(use_cat_names=True),
    LogisticRegression(max_iter=1000)
)

model.fit(X_train,y_train)

# get the test and trin accuracy
acc_train =accuracy_score(y_train,model.predict(X_train))
acc_test =model.score(X_test,y_test)

print("Training Accuracy:", round(acc_train, 2))
print("Test Accuracy:", round(acc_test, 2))

y_train_pred_proba =model.predict_proba(X_train)
print(y_train_pred_proba[:5])

"""## Feature Importances"""

features =model.named_steps["onehotencoder"].get_feature_names()
importances =model.named_steps["logisticregression"].coef_[0]
feat_imp=pd.Series(np.exp(importances),index=features).sort_values()
feat_imp.head()

#Get Odds Ratios
odds_ratios=feat_imp
odds_ratios.head()

# Horizontal bar chart, five largest coefficients
odds_ratios.tail().plot(kind="barh");

# Horizontal bar chart, five smallest coefficients
odds_ratios.head().plot(kind="barh");

"""# Decision Tree"""

model_t =make_pipeline(
    OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1),DecisionTreeClassifier(max_depth=5,random_state=42)
)
model_t.fit(X_train,y_train)

#Get the train and test Accuracy score
acc_train_t =model_t.score(X_train,y_train)
acc_test_t =model_t.score(X_test,y_test)

print("Training Accuracy:", round(acc_train_t, 2))
print("Validation Accuracy:", round(acc_test_t, 2))

"""## Tune Hyperparameter:Tree Depth"""

tree_depth =model_t.named_steps["decisiontreeclassifier"].get_depth()
print("Tree Depth:", tree_depth)

depth_hyperparams =range(1,10)

# Create empty lists for training and validation accuracy scores
training_acc = []
validation_acc = []

for d in depth_hyperparams:
    # Create model with `max_depth` of `d`
    test_model =make_pipeline(OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1),
                             DecisionTreeClassifier(max_depth=d,random_state=42)
                             )
    # Fit model to training data
    test_model.fit(X_train, y_train)
    # Calculate training accuracy score and append to `training_acc`
    training_acc.append(test_model.score(X_train,y_train))
    # Calculate validation accuracy score and append to `training_acc`
    validation_acc.append(test_model.score(X_test,y_test))

print("Training Accuracy Scores:", training_acc[:3])
print("Validation Accuracy Scores:", validation_acc[:3])

# Plot `depth_hyperparams`, `training_acc`
plt.plot(depth_hyperparams,training_acc,label='training')
plt.plot(depth_hyperparams,validation_acc,label='validation')
plt.legend();

"""## From the plot , it looks like the model perfoms well with a depth of 5"""

test_model =make_pipeline(OrdinalEncoder(handle_unknown="use_encoded_value", unknown_value=-1),
                             DecisionTreeClassifier(max_depth=5,random_state=42)
                             )
# Fit model to training data
test_model.fit(X_train, y_train)
# Calculate training accuracy score
training_acc=test_model.score(X_train,y_train)
# Calculate validation accuracy score
validation_acc=test_model.score(X_test,y_test)

print("Training Accuracy Scores:", training_acc)
print("Validation Accuracy Scores:", validation_acc)

"""### Model Accuracy remained the same meaning that the depth parameter is good."""

# Create larger figure
fig, ax = plt.subplots(figsize=(25, 12))
# Plot tree
plot_tree(
    decision_tree=test_model.named_steps['decisiontreeclassifier'],
    feature_names=list(X_train.columns),
    filled=True,  # Color leaf with class
    rounded=True,  # Round leaf edges
    proportion=True,  # Display proportion of classes in leaf
    max_depth=3,  # Only display first 3 levels
    fontsize=12,  # Enlarge font
    ax=ax,  # Place in figure axis
);

features =X_train.columns
importances =test_model.named_steps["decisiontreeclassifier"].feature_importances_

print("Features:", features[:3])
print("Importances:", importances[:3])

"""## Get Feature Importances"""

feat_imp =pd.Series(importances,index=features).sort_values()
feat_imp.head()

# Create horizontal bar chart
feat_imp.plot(kind="barh")

"""# Conclusion

### Looking at the perfomance of the two models, we see that the Logistic regression model does a better job at predicting 'Credit Default' with an accuracy score of of 0.88 whilst that of the decsion tree is 0.74
"""